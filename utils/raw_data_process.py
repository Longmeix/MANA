import json
import pickle
import random
import networkx as nx
import os
import numpy as np

def read_pickle(infile):
    with open(infile, 'rb') as f:
        return pickle.load(f)

def write_pickle(obj, outfile, protocol=-1):
    with open(outfile, 'wb') as f:
        pickle.dump(obj, f, protocol=protocol)

# # ------- DBLP17 - DBLP19 -----------
# data_dir = '../../dataset/DBLP/'
# g1, g2 = pickle.load(open(data_dir + 'raw/networks', 'rb'))
# adj_s = nx.to_scipy_sparse_matrix(g1).astype(np.float)
# adj_t = nx.to_scipy_sparse_matrix(g2).astype(np.float)
# write_pickle(adj_s, data_dir + 'adj_s.pkl')
# write_pickle(adj_t, data_dir + 'adj_t.pkl')
#
# anchors = json.load(open(data_dir + 'raw/anchors.txt', 'r'))
# s_num = adj_s.shape[0]
# anchors = [(i[0], i[1] - s_num) for i in anchors]
# random.seed(2022)
# random.shuffle(anchors)
# # train_ratio = 0.5
# for train_ratio in [.1, .2, .3, .4]:
#     train_num = int(train_ratio * len(anchors))
#     train_anchors, test_anchors = anchors[: train_num], anchors[train_num :]
#     write_pickle((train_anchors, test_anchors), data_dir + f'links_{train_ratio}.pkl')
#
# # ----- Allmovie - Tmdb ----
# data_dir = '../../dataset/Allmv_Tmdb/'
# edge_s = np.load(data_dir + 'allmv/edges.edgelist.npy')
# edge_t = np.load(data_dir + 'tmdb/edges.edgelist.npy')
# g1 = nx.from_edgelist(edge_s, create_using=nx.DiGraph())
# g2 = nx.from_edgelist(edge_t, create_using=nx.DiGraph())
# adj_s = nx.to_scipy_sparse_matrix(g1)
# adj_t = nx.to_scipy_sparse_matrix(g2)
# write_pickle(adj_s, data_dir + 'adj_s.pkl')
# write_pickle(adj_t, data_dir + 'adj_t.pkl')
#
# anchors = []
# with open(data_dir + 'dictionaries/groundtruth', 'r') as file:
#     for line in file:
#         src, trg = line.strip().split()
#         anchors.append((int(src), int(trg)))
# random.seed(2022)
# random.shuffle(anchors)
# train_ratio = 0.5
# train_num = int(train_ratio * len(anchors))
# train_anchors, test_anchors = anchors[: train_num], anchors[train_num :]
# write_pickle((train_anchors, test_anchors), data_dir + f'links_{train_ratio}.pkl')


# # ----- FT ----
# data_dir = '../../dataset/FT/'
# anchors = read_pickle(data_dir + 'links_1.0.pkl')
# random.seed(2022)
# random.shuffle(anchors)
# # for train_ratio in [.1, .2, .3, .4]:
# train_ratio = 0.6
# train_num = int(train_ratio * len(anchors))
# train_anchors, test_anchors = anchors[: train_num], anchors[train_num :]
# write_pickle((train_anchors, test_anchors), data_dir + f'links_{train_ratio}.pkl')


# ------ Transfer D-W-15K-V1 to NA format ------
def load_openea(file_name, ent_begin_id, rel_begin_id, rel2id=dict()):
    '''
        Load OpenEA dataset: https://github.com/nju-websoft/OpenEA
        this function load the entity and relation information seperately
        but notice that when this is the second KG the rel2id is generated by the first KG, and is dict() when it is the first execution
        this is simply because the relations are shared between different KG, the entity is not.
    '''
    ent2id, id2ent, ent_ids = dict(), dict(), list()
    rel2id, id2rel, rel_ids = rel2id, dict(), list()
    triples, entity, rel = list(), set(), set()
    ent_id, rel_id = ent_begin_id, rel_begin_id
    with open(file_name, 'r', encoding='utf-8') as f:
        data = f.read().strip().split("\n")
        data = [i.split("\t") for i in data]
        for item in data:
            entity.add(item[0])
            entity.add(item[2])
            rel.add(item[1])
            triples.append([item[0], item[1], item[2]])
            if item[0] not in ent2id:
                ent2id[item[0]] = ent_id
                id2ent[ent_id] = item[0]
                ent_ids.append(ent_id)
                ent_id += 1
            if item[2] not in ent2id:
                ent2id[item[2]] = ent_id
                id2ent[ent_id] = item[2]
                ent_ids.append(ent_id)
                ent_id += 1
            if item[1] not in rel2id:
                rel2id[item[1]] = rel_id
                id2rel[rel_id] = item[1]
                rel_ids.append(rel_id)
                rel_id += 1
    return ent2id, id2ent, rel2id, id2rel, ent_ids, rel_ids, entity, rel, triples


def load_alignment_pair(file_name):
    '''Load alignment pair information for training and tesing phase'''
    alignment_pair = list()
    for line in open(file_name, 'r'):
        e1, e2 = line.split()
        alignment_pair.append((e1, e2))
        # alignment_pair.append((ent2id[e1], ent2id[e2]))
    return alignment_pair


def writeLink2file(links, fpath):
    with open(fpath, 'w') as f:
        for l in links:
            link = str(l[0]) + '\t' + str(l[1]) + '\n'
            f.write(link)


file_dir = "../dataset/D_W_15K_V1"
save_NA_file = file_dir + '/NA_format'
save = True
if not os.path.exists(save_NA_file):
    os.mkdir(save_NA_file)
ent2id1, id2ent1, rel2id1, id2rel1, kg1_ent_ids, kg1_rel_ids, entity1, rel1, triples1 = load_openea(f"{file_dir}/rel_triples_1", ent_begin_id=0, rel_begin_id=0)
ent2id2, id2ent2, rel2id2, id2rel2, kg2_ent_ids, kg2_rel_ids, entity2, rel2, triples2 = load_openea(f"{file_dir}/rel_triples_2", ent_begin_id=0, rel_begin_id=len(kg1_rel_ids), rel2id=rel2id1)
ent2id = {**ent2id1, **ent2id2}
id2ent = {**id2ent1, **id2ent2}
rel2id = {**rel2id1, **rel2id2}
# id2rel = {**id2rel1, **id2rel2} this code is wrong!!!
id2rel = {v:k for k,v in rel2id.items()}

if save:
    with open(save_NA_file + '/edgelist_s.txt', "w", encoding="utf-8") as f:
        for item in triples1:
            f.write(f'{ent2id1[item[0]]} {ent2id1[item[2]]}\n')
    with open(save_NA_file + '/edgelist_t.txt', "w", encoding="utf-8") as f:
        for item in triples2:
            f.write(f'{ent2id2[item[0]]} {ent2id2[item[2]]}\n')

    edge_s = np.loadtxt(save_NA_file + '/edgelist_s.txt')
    edge_t = np.loadtxt(save_NA_file + '/edgelist_t.txt')
    g1 = nx.from_edgelist(edge_s, create_using=nx.DiGraph())
    g2 = nx.from_edgelist(edge_t, create_using=nx.DiGraph())
    adj_s = nx.to_scipy_sparse_matrix(g1)
    adj_t = nx.to_scipy_sparse_matrix(g2)
    write_pickle(adj_s, save_NA_file + '/adj_s.pkl')
    write_pickle(adj_t, save_NA_file + '/adj_t.pkl')

    all_links = load_alignment_pair(f"{file_dir}/ent_links")  # links in text
    random.seed(2023)
    random.shuffle(all_links)
    train_ratio = 0.5
    train_num = int(train_ratio * len(all_links))
    train_anchors, test_anchors = all_links[: train_num], all_links[train_num :]
    links_file = save_NA_file + f'/train_ratio_{train_ratio}'
    if not os.path.exists(links_file):
        os.mkdir(links_file)
    writeLink2file(train_anchors, links_file + '/train_links')
    writeLink2file(test_anchors, links_file + '/test_links')

    # links in id (number)
    train_links_id, test_links_id = [], []
    for e1, e2 in train_anchors:
        train_links_id.append((ent2id1[e1], ent2id2[e2]))
    for e1, e2 in test_anchors:
        test_links_id.append((ent2id1[e1], ent2id2[e2]))
    write_pickle((train_links_id, test_links_id), save_NA_file + f'/links_{train_ratio}.pkl')


# # --- NA format dataset transfer to EA format ---
# dataset_name = 'FT'
# train_ratio = 0.5
# data_dir = f'./dataset/{dataset_name}'
# save_ea_dir = data_dir + '/EA_data'
# save_link_dir = save_ea_dir + f'train_ratio_{train_ratio}'
# if not os.path.exists(save_link_dir):
#     os.makedirs(save_link_dir)

# adj_path = data_dir + '/adj_{}.pkl'
# for i, net in enumerate(('s', 't')):
#     adj_mat = read_pickle(adj_path.format(net))
#     nodes = list(zip(*(adj_mat.nonzero())))
#     with open(save_ea_dir + f'/rel_triples_{i + 1}', 'w', encoding='utf-8') as f:
#         for node in nodes:
#             triples = f'{node[0]}\tneighbor\t{node[1]}\n'
#             f.write(triples)

